{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe89bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading process file: Recipedb_general.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvish\\AppData\\Local\\Temp\\ipykernel_13076\\318265874.py:45: DtypeWarning: Columns (2,3,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_proc = pd.read_csv(PROCESS_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 270 unique processes after cleaning.\n",
      "[INFO] Preparing corpus for Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hvish\\AppData\\Local\\Temp\\ipykernel_13076\\318265874.py:110: DtypeWarning: Columns (2,6,7,8,11,12,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_steps = pd.read_csv(STEPS_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sentences for W2V: steps=50000, processes=270\n",
      "[INFO] Training Word2Vec (this can take a minute)...\n",
      "[INFO] Word2Vec trained. Vocab size: 20775\n",
      "[INFO] Process embeddings shape: (270, 100)\n",
      "[INFO] Clustering unique processes...\n",
      "[INFO] Saved cluster assignments to CSV.\n",
      "[INFO] Wrote human-readable summary.\n",
      "[INFO] Creating PCA plots for each clustering method...\n",
      "[INFO] Saved plot: Process_Clustering_Results\\processes_pca_KMeans.png\n",
      "[INFO] Saved plot: Process_Clustering_Results\\processes_pca_Agglo_or_MBK.png\n",
      "[INFO] Saved plot: Process_Clustering_Results\\processes_pca_DBSCAN.png\n",
      "[INFO] Saved plot: Process_Clustering_Results\\processes_pca_GMM.png\n",
      "\n",
      "[ALL DONE]\n",
      "Results folder: Process_Clustering_Results\n",
      " - CSV: unique_processes_clusters.csv\n",
      " - Summary: process_cluster_summary.txt\n",
      " - PCA plots: processes_pca_<method>.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "STEPS_FILE = 'RecipeDB2_general_final.csv'   # file used to train Word2Vec (contains 'steps')\n",
    "PROCESS_FILE = 'Recipedb_general.csv'       # file containing 'Processes' column\n",
    "OUTPUT_FOLDER = 'Process_Clustering_Results'\n",
    "ROW_LIMIT = 50000                            # if you want to limit W2V training rows; set None to use all\n",
    "W2V_DIM = 100\n",
    "NUM_CLUSTERS = 8                             # choose as appropriate for process clustering\n",
    "DBSCAN_EPS = 0.7\n",
    "DBSCAN_MIN_SAMPLES = 2\n",
    "AGGLO_THRESHOLD = 2000                       # if unique processes > this, skip Agglo for memory\n",
    "ANNOTATE = True                              # annotate points with label (set False to hide)\n",
    "\n",
    "# Simple stopwords to remove accidental non-process words\n",
    "STOPWORDS = set([\n",
    "    'and','or','the','a','an','in','on','of','with','to','for','by','at','from','into',\n",
    "    'grated','chopped','sliced'  # note: if these are valid processes you want keep them remove from STOPWORDS\n",
    "])\n",
    "\n",
    "# Patterns considered valid process-like tokens (letters, spaces, hyphens)\n",
    "VALID_PROCESS_RE = re.compile(r'^[a-z][a-z0-9\\-\\s]+$')\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load processes file and extract unique process tokens\n",
    "# -----------------------------\n",
    "print(\"[INFO] Loading process file:\", PROCESS_FILE)\n",
    "df_proc = pd.read_csv(PROCESS_FILE)\n",
    "\n",
    "# find 'Processes' column case-insensitively\n",
    "proc_col = None\n",
    "for c in df_proc.columns:\n",
    "    if c.strip().lower() == 'processes':\n",
    "        proc_col = c\n",
    "        break\n",
    "if proc_col is None:\n",
    "    raise ValueError(\"No 'Processes' column found in the processes file.\")\n",
    "\n",
    "# Collect raw tokens from column splitting on '||'\n",
    "raw_processes = []\n",
    "for val in df_proc[proc_col].dropna().astype(str):\n",
    "    parts = [p.strip() for p in val.split('||') if p.strip()]\n",
    "    raw_processes.extend(parts)\n",
    "\n",
    "# Normalize and filter processes\n",
    "def clean_process_token(tok):\n",
    "    tok = tok.strip().lower()\n",
    "    # remove leading/trailing punctuation\n",
    "    tok = re.sub(r'^[^\\w]+|[^\\w]+$', '', tok)\n",
    "    return tok\n",
    "\n",
    "cleaned = [clean_process_token(p) for p in raw_processes]\n",
    "\n",
    "# Keep only tokens that look like process words and are not in STOPWORDS\n",
    "filtered = []\n",
    "for p in cleaned:\n",
    "    if len(p) < 2: \n",
    "        continue\n",
    "    # filter out obvious non-process garbage: urls, numbers-only, too-long\n",
    "    if p.isdigit() or len(p) > 40:\n",
    "        continue\n",
    "    if p in STOPWORDS:\n",
    "        continue\n",
    "    if not VALID_PROCESS_RE.match(p):\n",
    "        # allow multi-word like 'pan-fry' or 'lightly stir' by removing weird punctuation\n",
    "        p2 = re.sub(r'[^a-z0-9\\-\\s]', '', p)\n",
    "        if VALID_PROCESS_RE.match(p2):\n",
    "            p = p2\n",
    "        else:\n",
    "            continue\n",
    "    # final length check of words (avoid single-letter nonsense)\n",
    "    toks = p.split()\n",
    "    if any(len(t) <= 1 for t in toks):\n",
    "        # keep tokens like 'pan fry' where words length>1; skip if any 1-char token\n",
    "        if not all(len(t) > 1 for t in toks):\n",
    "            continue\n",
    "    filtered.append(p)\n",
    "\n",
    "unique_processes = sorted(list(set(filtered)))\n",
    "print(f\"[INFO] Found {len(unique_processes)} unique processes after cleaning.\")\n",
    "\n",
    "# Save list for inspection\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"unique_processes_raw.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in unique_processes:\n",
    "        f.write(p + \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Prepare Word2Vec training data:\n",
    "#    train on 'steps' + also include each process as its own sentence so W2V learns process tokens\n",
    "# -----------------------------\n",
    "print(\"[INFO] Preparing corpus for Word2Vec...\")\n",
    "\n",
    "df_steps = pd.read_csv(STEPS_FILE)\n",
    "\n",
    "# optionally limit rows for speed\n",
    "if ROW_LIMIT:\n",
    "    df_steps = df_steps.head(ROW_LIMIT)\n",
    "\n",
    "# Tokenize steps (basic)\n",
    "def tokenize_step_text(s):\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    s = str(s).lower()\n",
    "    return re.findall(r'\\b\\w+\\b', s)\n",
    "\n",
    "sentences = df_steps['steps'].fillna('').apply(tokenize_step_text).tolist()\n",
    "\n",
    "# Add process tokens as individual 'sentences' to help Word2Vec learn them\n",
    "process_sentences = [re.findall(r'\\b\\w+\\b', p) for p in unique_processes]\n",
    "sentences_for_w2v = sentences + process_sentences\n",
    "\n",
    "print(f\"[INFO] Sentences for W2V: steps={len(sentences)}, processes={len(process_sentences)}\")\n",
    "\n",
    "# Train Word2Vec\n",
    "print(\"[INFO] Training Word2Vec (this can take a minute)...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences_for_w2v,\n",
    "    vector_size=W2V_DIM,\n",
    "    window=8,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "print(\"[INFO] Word2Vec trained. Vocab size:\", len(w2v_model.wv))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Convert each unique process into an embedding\n",
    "# -----------------------------\n",
    "def embed_process_text(text, model, dim):\n",
    "    # multi-word processes -> average token vectors\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "process_embeddings = np.vstack([embed_process_text(p, w2v_model, W2V_DIM) for p in unique_processes])\n",
    "print(\"[INFO] Process embeddings shape:\", process_embeddings.shape)\n",
    "\n",
    "# If all-zero embeddings exist (no tokens in vocab), warn\n",
    "zero_counts = np.sum(np.all(process_embeddings == 0, axis=1))\n",
    "if zero_counts > 0:\n",
    "    print(f\"[WARN] {zero_counts} process(es) produced zero embeddings (no tokens in W2V vocab).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Clustering the unique processes\n",
    "# -----------------------------\n",
    "print(\"[INFO] Clustering unique processes...\")\n",
    "\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=20)\n",
    "labels_kmeans = kmeans.fit_predict(process_embeddings)\n",
    "\n",
    "# Decide whether to run Agglomerative\n",
    "run_agglo = len(unique_processes) <= AGGLO_THRESHOLD\n",
    "if run_agglo:\n",
    "    agglo = AgglomerativeClustering(n_clusters=NUM_CLUSTERS, linkage='ward')\n",
    "    labels_agglo = agglo.fit_predict(process_embeddings)\n",
    "else:\n",
    "    print(f\"[INFO] Number of processes ({len(unique_processes)}) > {AGGLO_THRESHOLD}; skipping Agglomerative.\")\n",
    "    # Use MiniBatchKMeans as a memory-friendly substitute and label it 'Agglo_replacement'\n",
    "    mbk = MiniBatchKMeans(n_clusters=NUM_CLUSTERS, random_state=42, batch_size=256)\n",
    "    labels_agglo = mbk.fit_predict(process_embeddings)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES, metric='euclidean')\n",
    "labels_dbscan = dbscan.fit_predict(process_embeddings)\n",
    "\n",
    "# GMM\n",
    "gmm = GaussianMixture(n_components=NUM_CLUSTERS, random_state=42)\n",
    "labels_gmm = gmm.fit_predict(process_embeddings)\n",
    "\n",
    "# Build result DataFrame\n",
    "df_procs = pd.DataFrame({\n",
    "    'Process': unique_processes,\n",
    "    'Embedding': [json.dumps(vec.tolist()) for vec in process_embeddings],\n",
    "    'KMeans': labels_kmeans,\n",
    "    'Agglo_or_MBK': labels_agglo,\n",
    "    'DBSCAN': labels_dbscan,\n",
    "    'GMM': labels_gmm\n",
    "})\n",
    "\n",
    "df_procs.to_csv(os.path.join(OUTPUT_FOLDER, \"unique_processes_clusters.csv\"), index=False)\n",
    "print(\"[INFO] Saved cluster assignments to CSV.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Human-readable cluster summaries\n",
    "# -----------------------------\n",
    "def summary_text_for_col(df, col):\n",
    "    s = f\"\\n=== {col} ===\\n\"\n",
    "    for c in sorted(df[col].unique()):\n",
    "        subset = df[df[col] == c]['Process'].tolist()\n",
    "        s += f\"\\nCluster {c} ({len(subset)}):\\n\"\n",
    "        s += \", \".join(subset[:200]) + (\"\\n\" if len(subset)<=200 else \" ...\\n\")\n",
    "    return s\n",
    "\n",
    "summary = \"PROCESS CLUSTER SUMMARY\\n\"\n",
    "for col in ['KMeans', 'Agglo_or_MBK', 'DBSCAN', 'GMM']:\n",
    "    summary += summary_text_for_col(df_procs, col)\n",
    "\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"process_cluster_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary)\n",
    "print(\"[INFO] Wrote human-readable summary.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) PCA + plotting for every method\n",
    "# -----------------------------\n",
    "print(\"[INFO] Creating PCA plots for each clustering method...\")\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "proc_2d = pca2.fit_transform(process_embeddings)\n",
    "\n",
    "plot_methods = {\n",
    "    'KMeans': labels_kmeans,\n",
    "    'Agglo_or_MBK': labels_agglo,\n",
    "    'DBSCAN': labels_dbscan,\n",
    "    'GMM': labels_gmm\n",
    "}\n",
    "\n",
    "# function for nice scatter\n",
    "def plot_process_clusters(xy, labels, title, outpath, annotate=ANNOTATE):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    palette = sns.color_palette('tab10') if len(set(labels)) <= 10 else sns.color_palette('husl', n_colors=len(set(labels)))\n",
    "    scatter = plt.scatter(xy[:,0], xy[:,1], c=labels, cmap=None, s=120)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    # annotate labels (best for smaller sets)\n",
    "    if annotate and len(unique_processes) <= 300:\n",
    "        for i, txt in enumerate(unique_processes):\n",
    "            plt.annotate(txt, (xy[i,0], xy[i,1]), textcoords=\"offset points\", xytext=(3,3), fontsize=8)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved plot: {outpath}\")\n",
    "\n",
    "for method_name, labels in plot_methods.items():\n",
    "    out = os.path.join(OUTPUT_FOLDER, f\"processes_pca_{method_name}.png\")\n",
    "    plot_process_clusters(proc_2d, labels, f\"Processes clustered by {method_name}\", out, annotate=ANNOTATE)\n",
    "\n",
    "print(\"\\n[ALL DONE]\")\n",
    "print(\"Results folder:\", OUTPUT_FOLDER)\n",
    "print(\" - CSV: unique_processes_clusters.csv\")\n",
    "print(\" - Summary: process_cluster_summary.txt\")\n",
    "print(\" - PCA plots: processes_pca_<method>.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
